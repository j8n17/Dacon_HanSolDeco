# 🚧 프로젝트 상세 내용 (DETAILS)

이 문서는 [README.md](https://github.com/j8n17/Dacon_HanSolDeco/blob/main/README.md)에서 간략하게 다룬 내용들에 대한 더 깊이 있는 분석, 실험 과정, 문제 해결 기록 등을 상세히 기술합니다.

## 목차

1.  [EDA 분석 결과](#1-eda-분석-결과)
2.  [프로젝트 접근 방식 및 전략 변화](#2-프로젝트-접근-방식-및-전략-변화)
3.  [모델 선정 과정](#3-모델-선정-과정)
4.  [프롬프트 엔지니어링](#4-프롬프트-엔지니어링)
5.  [프로젝트 관리](#5-프로젝트-관리)
6.  [에러 및 문제 해결 기록](#6-에러-및-문제-해결-기록)
7.  [README 작성 요령](#7-readme-작성-요령)

---

## 1. EDA 분석 결과

탐색적 데이터 분석(EDA) 과정에서 얻은 주요 인사이트는 다음과 같습니다.

### 1.1. 사고 설명문과 답변의 매칭 분석

* **사고 설명문 기준:** 훈련 데이터의 '사고 설명문' 텍스트를 임베딩 벡터로 변환한 후, PCA(주성분 분석)를 이용해 3차원으로 시각화했습니다. 시각화된 벡터 공간에서, 유사한 '재발방지대책 및 향후조치계획'(답변)을 가진 데이터 25개를 선정하여 동일한 색상으로 표시했습니다.
* **답변 기준:** 반대로, '재발방지대책 및 향후조치계획'(답변) 텍스트를 임베딩 벡터로 변환하고 3차원으로 시각화했습니다. 이 공간에서는 유사한 '사고 설명문'을 가진 데이터 25개를 동일한 색상으로 표시했습니다.
* **결론:** 두 가지 분석을 통해, **의미적으로 유사한 사고 설명문이라도 매우 다양한 형태의 답변(조치 계획)이 존재**할 수 있음을 확인했습니다. 마찬가지로, **유사한 답변(조치 계획)이라도 서로 다른 다양한 사고 설명문(사고 상황)에서 도출**될 수 있다는 점을 파악했습니다. 이는 RAG 과정에서 단순히 유사한 사고 사례를 찾는 것만으로는 최적의 답변을 생성하기 어려울 수 있음을 시사합니다.
* [사고 설명문 임베딩 시각화](https://github.com/j8n17/Dacon_HanSolDeco/blob/main/EDA/sim_answer_embed_visualization.html), [답변 임베딩 시각화](https://github.com/j8n17/Dacon_HanSolDeco/blob/main/EDA/sim_question_embed_visualization.html) 참고.

### 1.2. '사고 원인' 컬럼의 정보 혼재

* 데이터의 '사고 원인' 컬럼을 분석한 결과, 명확한 '사고 원인' 정보 외에도 사고 발생 배경, 사고 종류, 보험 관련 의견, 신청인의 주장, 기존 질병 이력 등 다양한 부가 정보가 길게 혼재되어 있는 경우가 많음을 확인했습니다.
* 이는 RAG 검색 시 '사고 원인' 컬럼을 직접 쿼리로 사용할 경우, 핵심 원인과 관련성이 낮거나 불필요한 정보까지 검색될 수 있는 잠재적 문제를 야기할 수 있습니다. 따라서 핵심 원인만 추출하여 사용하는 것이 검색 품질 향상에 중요했습니다.
* **예시:** 아래는 가장 긴 '사고 원인' 텍스트와 [30-colab-structured_description.ipynb](https://github.com/j8n17/Dacon_HanSolDeco/blob/main/preprocess/30-colab-structured_description.ipynb)에서 LLM을 통해 핵심 원인만 정제한 결과입니다.

    **가장 긴 '사고 원인' 텍스트:**

    > 신청인의 요양급여신청 관련하여 사실에 근거하여 아래와 같이 보험가입자의견서를 작성하여 근로복지공단 통영지사에 제출함(첨부 보험가입자의견서 참조)1. 신청인이 당사현장에서 행한 업무는 작업내용도 단순하며 특별히 허리에 부담이 되지 않는 등 업무량이나 업무강도가 일용근로자들이 일반적으로 하는 정도에 불과할 뿐이었다는 점2. 신청인은 신호수 보조공으로서의 업무내용은 숙달된 신호수의 지시에 따라 크레인 작업시 인양물체 슬링벨트 설치 및 신호전달과 현장 정리정돈 등의 단순 보조업무만수행하였을 뿐, 특별히 허리에 부담이 가는 작업은 아니라는 점3. 신청인이 주장하는 상병은 장기간에 걸쳐서 서서히 발현된 전형적인 퇴행성(만성) 허리 질환의 일환임이 명백하고, 자연경과적인 퇴행성(진구성) 변화에 의한 기존질환인바당사 현장의 업무와의 인과관계를 인정할 수 없다는 점4. 신청인이 기존 질병을 “2019. 06.24. M5243 경추통,경흉추부” 은폐하고 경추통,경흉추부를 연계하여 허리에 통증을 느꼈다고 주장하며 마치 “요추의 염좌”가 현장에서 발생한업무상 사고라고 주장하고 있는 점5. 업무와 재해간의 상당인과관계에 대한 입증책임은 신청인에게 있음에도 불구하고, 신청인의 경우 본 사안과 관련하여 인과관계를 입증할 만한 객관적인 입증자료를 제출하지 못하고있다는 점 등의 여러 정황을 종합하여 볼 때 당사로서는 신청인이 주장하는 재해를 업무상 재해로 인정할 수 없다는 내용으로  보험가입자의견서를 근로복지공단 통영지사에 제출함

    **정제된 '사고 원인' (LLM 추출 결과):**
    
    > 신청인의 허리 통증이 기존 질환의 자연스러운 퇴행성 변화에 의한 것으로 판단됨.

---

## 2. 프로젝트 접근 방식 및 전략 변화

프로젝트 초기부터 최종 제출까지 접근 방식과 전략은 다음과 같은 과정을 거쳐 발전했습니다.

1.  **초기 접근: 대표 문장 활용 (0.5465)**
    * 대회 게시판에서 특정 문장 하나로 예측해도 점수가 잘 나온다는 정보를 접했습니다.
    * 이에 착안하여, 훈련 데이터의 모든 정답 문장('재발방지대책 및 향후조치계획') 임베딩 벡터 간의 코사인 유사도를 계산하여 평균 유사도가 가장 높은 하나의 문장을 선정했습니다. 이를 **대표 문장** 이라고 명명하고, 모든 테스트 데이터에 대해 이 대표 문장을 제출하는 방식을 시도했습니다. 예상 외로 높은 초기 점수를 얻었습니다. ([find_general_sentence.ipynb](https://github.com/j8n17/Dacon_HanSolDeco/blob/main/EDA/find_general_sentence.ipynb) 참고)

    **선정된 대표 문장**
    
    > 작업전 안전교육 강화 및 작업장 위험요소 점검을 통한 재발 방지와 안전관리 교육 철저를 통한 향후 조치 계획.

2.  **군집화 시도 및 실패**
    * 대표 문장 방식의 한계를 인지하고, 주어진 사고 정보('사고 설명문')를 기반으로 유사한 사고끼리 군집화를 시도했습니다. 각 군집별로 특화된 대표 문장을 찾거나, 군집 정보를 RAG 검색에 활용할 수 있지 않을까 기대했습니다.
    * 하지만, 군집화가 잘 되지 않았고 '사고 설명문' 임베딩 벡터를 시각화하고 PCA의 누적 분산 비율 등을 분석한 결과, 데이터가 명확한 군집 구조를 가지기보다는 넓게 퍼져 있어 군집화가 효과적이지 않다는 결론을 내렸습니다.

3.  **RAG 도입 및 개선**
    * **기본 RAG (0.4385):** 군집화 대신, 각 사고 정보에 맞는 유사 사례를 직접 검색하여 LLM에게 참고하도록 하는 RAG(Retrieval-Augmented Generation) 방식으로 전환했습니다. 초기에는 '사고 원인' 컬럼을 쿼리로 사용하고, '사고 설명문'으로 검색 결과를 재정렬(Reranking)하여 상위 5개 사례를 LLM에 제공했습니다.
    * **임베딩 모델 변경:**
        * 초기에는 임베딩 모델로 `jhgan/ko-sbert-sts`을 사용하였으나, RAG 검색 및 파이프라인 전반의 성능 향상을 위해 한국어 벤치마크 등에서 우수한 성능을 보인 `upskyy/bge-m3-korean` 모델로 변경하여 사용했습니다.
    * **RAG + Base Answer (0.4701):** 기본 RAG 방식의 문제점을 개선하기 위해, LLM 프롬프트에 초기 접근에서 찾았던 **대표 문장**을 `base_answer`로 함께 제공했습니다. 이는 LLM이 답변을 생성할 때 참고할 기본적인 방향성을 제시해주어 성능 향상을 가져왔습니다. 검색 사례 수는 3개로 줄였습니다.
    * **JSON 정보 활용 RAG (0.4899):** EDA에서 확인된 '사고 원인' 컬럼의 정보 혼재 문제를 해결하기 위해, LLM을 사용하여 '사고 원인', '발생 배경', '사고 종류' 등 구조화된 정보를 JSON 형식으로 추출했습니다. 이렇게 정제된 '사고 원인'을 쿼리로 사용하고, '구조화된 설명문'으로 재정렬하여 RAG의 검색 품질을 높였습니다.
    * **다양한 답변 제공 RAG (0.4951):** 단순히 쿼리와 가장 유사한 사례만 제공하는 것보다, LLM에게 더 다양한 관점의 답변을 참고하도록 유도하는 것이 좋을 수 있다고 판단했습니다. 검색된 결과들의 답변과 '대표 문장' 간의 유사도를 기준으로 재정렬하여, 유사도가 가장 높은 답변, 중간인 답변, 낮은 답변을 가진 3개의 사례를 LLM에게 제공했습니다. 이는 LLM이 더 폭넓은 정보를 바탕으로 답변을 생성하게 하여 소폭의 성능 개선을 이끌었습니다.

4.  **최종 파이프라인: 답변 생성 및 재선정 (BoN)**
    * 최종적으로는 '다양한 답변 제공 RAG'를 통해 얻은 3개의 예시와 `base_answer`를 바탕으로 LLM이 N개의 후보 답변을 생성하도록 했습니다.
    * 생성된 N개의 답변들을 다시 RAG 단계에서 검색된 상위 25개 답변들과 코사인 유사도를 비교하여, 가장 높은 평균 유사도를 보이는 답변 1개를 최종 결과로 선정하는 Best-of-N(BoN) 방식을 채택했습니다. ([80-local-answers_rerank.ipynb](https://github.com/j8n17/Dacon_HanSolDeco/blob/main/rag/80-local-answers_rerank.ipynb) 참고)

이러한 과정을 통해 초기 아이디어에서 출발하여 데이터 분석, 임베딩 모델 선정, RAG 전략 개선 및 최종 답변 선정 방식을 거쳐 점진적으로 성능을 개선하는 RAG 파이프라인을 구축하게 되었습니다.

---

## 3. 모델 선정 과정

최적의 LLM(Large Language Model)을 선정하기 위해 다음과 같은 과정을 거쳤습니다.

1.  **초기 모델:** 프로젝트 초기에는 `exaone3.5:7.8b` 모델을 사용했습니다. 성능은 준수했으나, 상업적 이용이 불가능하다는 제약 조건이 있어 대체 모델을 탐색하게 되었습니다.
2.  **후보 모델 탐색:** 상업적 이용이 가능하면서 성능이 우수한 모델들을 조사했습니다. `deepseek-r1:1.5b`, `gemma3`, `kwangsuklee/DeepSeek-R1-Distill-Qwen-7B-Multilingual-Q5_K_M` 등이 후보군에 포함되었습니다.
3.  **모델 테스트:**
    * `Deepseekr1`과 `Qwen` 기반 모델은 공개된 벤치마크 성능은 우수했으나, 실제 프로젝트 데이터(한국어 건설 사고 정보)에 적용했을 때 의도치 않게 중국어가 출력되는 문제가 발생했습니다.
    * Google의 `gemma3`(`27b`, `12b`, `4b`)를 테스트해본 결과, `gemma3:4b` 모델도 프롬프트 엔지니어링을 통해 충분히 만족스러운 품질의 답변을 생성할 수 있음을 확인했습니다.
4.  **최종 선정:** 리소스 효율성(모델 크기, 추론 속도)과 생성 결과의 품질을 종합적으로 고려하여 **`gemma3:4b`** 모델을 최종적으로 선정했습니다.

---

## 4. 프롬프트 엔지니어링

LLM의 성능을 극대화하고 원하는 결과를 얻기 위해 프롬프트 엔지니어링에 노력을 기울였습니다.

* **`prompt_test` 함수 활용:** 개발 과정에서 LLM의 응답 일관성 및 특정 프롬프트에 대한 결과 확인을 위해 `prompt_test` 함수를 구현하여 활용했습니다. 이 함수는 특정 인덱스(`idx`) 또는 랜덤 인덱스의 데이터를 사용하여 프롬프트를 구성하고 LLM의 답변을 출력합니다. 이를 통해 다양한 입력과 프롬프트 구성에 대한 모델의 반응을 빠르게 테스트하고 개선하는 데 활용했습니다.

    * **실행 예시:**
        아래는 `prompt_test` 함수를 사용하여 특정 프롬프트(`df`의 5668번 인덱스, `col`에 해당하는 프롬프트 템플릿)를 `chat_model`에 전달했을 때의 콘솔 출력 예시입니다.

        **입력 (콘솔 출력):**
        ```
        idx: 5668
        한국 건설 공사 안전 사고 관련 문장을 언어 모델에 사용하기 위해 전처리하려 합니다.
        사고가 발생한 배경과 핵심적인 피해 내용, 사고 원인을 도출하고, "발생 배경: , 사고 종류: , 사고 원인: " 형식으로 핵심적인 내용만 간결히 정리해주세요.
        추가적인 정보를 임의로 추론하거나 생성하지 말고, 원문에 주어진 정보만을 반영하세요.
        제공될 문장은 [문장: "사고 종류 (사고 설명 또는 원인)"] 형식으로 제공됩니다.
        문장: "설치작업 중 비계 관련 사고(작업자 부주의)"
        ```
        **LLM 답변 (콘솔 출력):**
        ```
        발생 배경: 설치작업 중
        사고 종류: 비계 관련 사고 (작업자 부주의)
        사고 원인: 작업자 부주의
        ```

    * **함수 코드:**
        <details>
        <summary><code>prompt_test</code> 함수 코드</summary>

        ```python
        def prompt_test(df, col, chat_model, idx=-1):
            if idx == -1:
                idx = random.randint(0, len(df) - 1)
            print(f"idx: {idx}")

            prompt = df.loc[idx, col]
            print(prompt)
            answer = run_ollama(prompt, chat_model)
            print(answer)

        prompt_test(test_df, 'correct_description', chain, idx=-1)
        ```

        또는 `model_query_test` 사용.

        ```python
        def make_structuring_prompt(row):
            prompt = f"""한국 건설 공사 안전 사고 관련 문장을 언어 모델에 사용하기 위해 전처리하려 합니다.
        사고가 발생한 배경과 핵심적인 피해 내용, 사고 원인을 도출하고, \"발생 배경, 사고 종류, 사고 원인\"을 json 형식으로 핵심적인 내용만 간결히 정리해주세요.
        추가적인 정보를 임의로 추론하거나 생성하지 말고, 원문에 주어진 정보만을 반영하세요.
        제공될 문장은 [문장: \"사고 종류 (사고 설명 또는 원인)\"] 형식으로 제공됩니다.
        문장: \"{row['correct_description']}\""""
            return prompt

        def model_query_test(df, prompt_func, chain, idx=-1):
            if idx == -1:
                random_idx = random.randint(0, len(df) - 1)
            else:
                random_idx = idx
            print(f"idx: {random_idx}")

            prompt = prompt_func(df.iloc[random_idx])
            print(prompt)

            if df.loc[random_idx, 'category_exists'] == "N":
                print("사고 종류 없으므로 그대로 사용.")
                return

            answer = run_ollama(prompt, chain)
            print(answer)

        model_query_test(train_df, make_structuring_prompt, chain, idx=-1)
        ```
        </details>

* **JSON 형식 활용:** 전처리 단계(`30-colab-structured_description.ipynb`) 및 RAG 답변 생성 단계(`60-colab-llm_summary.ipynb`)에서 LLM이 구조화된 정보를 정확히 추출하고 지정된 형식으로 답변하도록 프롬프트에 명시적으로 JSON 형식을 요구했습니다. 이는 후속 처리 과정을 자동화하고 오류를 줄이는 데 효과적이었습니다.

---

## 5. 프로젝트 관리

효율적인 프로젝트 진행을 위해 다음과 같은 도구와 방법을 활용했습니다.

* **Notion:** 프로젝트의 아이디어 구상부터 세부 작업 관리까지 전 과정을 Notion을 통해 체계적으로 관리했습니다. EDA, RAG, 개발 환경 구축 등 각 영역별로 아이디어, 진행 상황, 실험 결과 등을 기록하고 추적했습니다. ([노션 링크](https://www.notion.so/j8n17/1b51650beb7580cc9ef7c22dc52d2c05?pvs=4))
* **Google Drive 마운트:** Google Colab 환경과 로컬 환경에서 코드를 병행하여 개발했습니다. Google Drive를 로컬 환경에 마운트하여 모든 프로젝트 파일이 Colab 환경과 로컬 환경에 실시간으로 동기화되도록 설정했습니다. 이를 통해 동일한 코드를 여러 환경에서 중복으로 수정하는 번거로움을 없애고 코드의 일관성을 유지했습니다.

---

## 6. 에러 및 문제 해결 기록

프로젝트 진행 중 발생했던 주요 에러 및 문제점과 해결 과정은 다음과 같습니다.

* **Colab 환경에서의 알 수 없는 에러:**
    * **증상:** Colab에서 LLM(Ollama 기반)을 호출하는 코드 실행 중 간헐적으로 원인 불명의 에러가 발생하며 실행이 중단되었습니다.
    * **추정 원인:** Ollama 모델의 응답 지연 또는 불안정성으로 인해 예상치 못한 예외가 발생하는 것으로 추정했습니다.
    * **해결:** [utils.py](https://github.com/j8n17/Dacon_HanSolDeco/blob/main/rag/utils.py#L89) 내 LLM 호출 관련 함수에 예외 처리 로직을 추가했습니다. 사용자가 직접 실행을 중단하는 `KeyboardInterrupt`를 제외한 다른 모든 에러가 발생했을 경우, 해당 함수를 자동으로 재실행하도록 구현하여 안정성을 높였습니다.
* **느린 추론 속도:**
    * **문제:** 대량의 데이터에 대해 LLM 추론을 반복적으로 수행하는 과정(특히 답변 생성 및 재평가 단계)에서 상당한 시간이 소요되었습니다. 이는 주로 I/O 작업(모델 호출 및 응답 대기) 병목 현상 때문으로 판단되었습니다.
    * **해결:** Python의 `concurrent.futures.ThreadPoolExecutor`를 사용하여 여러 개의 LLM 요청을 병렬로 처리하도록 코드를 수정했습니다. 이를 통해 I/O 대기 시간을 효과적으로 줄여, 전체 추론 속도를 **약 2배** 가량 향상시킬 수 있었습니다.

---

## 7. README 작성 요령

프로젝트 내용을 효과적으로 전달하는 README.md를 작성하기 위해 다음과 같은 과정을 거쳤습니다.

1.  **초안 작성 (ChatGPT 활용):** ChatGPT를 활용해 프로젝트를 진행하며 고민했던 내용, 실행했던 과정, 실험 결과 등을 바탕으로 README의 전체적인 구조와 내용에 대한 초안을 작성했습니다.
2.  **벤치마킹 및 구체화 (Cursor, Gemini-2.5-pro 활용):**
    * Cursor IDE의 Agent를 활용했습니다. 전체 프로젝트 코드와 README 작성에 참고할만한 다른 프로젝트의 README 파일을 제공했습니다.
    * 이를 바탕으로 프로젝트의 특성에 맞는 섹션 구성, 기술 스택 표현 방식, 결과 요약 방식 등을 벤치마킹하여 초안을 구체화하고 개선하도록 요청했습니다.
3.  **검토 및 수정:** AI가 생성한 내용을 바탕으로, 추가할 내용 또는 삭제할 내용 등을 수정하고 보완하여 최종 README를 완성했습니다.